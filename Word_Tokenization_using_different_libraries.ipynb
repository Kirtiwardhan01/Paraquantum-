{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word Tokenization using different libraries.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNfSluWL345Bu+g+b6E2G24"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MI_0RN5b7HV8"
      },
      "source": [
        "###Word Tokenization Techniques in NLP\n",
        "\n",
        "What is word tokenization?\n",
        "\n",
        "Tokenization is breaking the raw text into small chunks. Tokenization breaks the raw text into words, sentences called **tokens.** These tokens help in understanding the context or developing the model for the NLP. The tokenization helps in interpreting the meaning of the text by analyzing the sequence of the words.\n",
        "\n",
        "    For example, the text “It is cold” can be tokenized into ‘It’, ‘is’, ‘cold’\n",
        "\n",
        "Tokenization can be done to either separate words or sentences.\n",
        "\n",
        "    If the text is split into words using some separation technique it is called word tokenization and \n",
        "    same separation done for sentences is called sentence tokenization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "BFCEW4t82yaj",
        "outputId": "16319e35-13c4-4f51-aa56-161288ac0e00"
      },
      "source": [
        "#Let's import relevamt libraries\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "#Simple example of word tokenization\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = 'Tokenization is breaking the raw text into small chunks'\n",
        "print(word_tokenize(text))   #print tokenized words\n",
        "\n",
        "'''You can see the sentence is broken down into tokens'''"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "['Tokenization', 'is', 'breaking', 'the', 'raw', 'text', 'into', 'small', 'chunks']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'You can see the sentence is broken down into tokens'"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMZCdeyp86tt"
      },
      "source": [
        "#### Example to tokenize multiple sentences first, and then the words contained in sentences of a text "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtraO_3Z7tyo",
        "outputId": "d36abc14-53b8-45ac-f749-9eb3f38ba985"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "text2 = \"Hello everyone. Welcome to the NLP Class. We'll learn about tokenization.\"\n",
        "for t in sent_tokenize(text2):\n",
        "\n",
        "    x =word_tokenize(t)\n",
        "    print(x) "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'everyone', '.']\n",
            "['Welcome', 'to', 'the', 'NLP', 'Class', '.']\n",
            "['We', \"'ll\", 'learn', 'about', 'tokenization', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5encoepH-0_6"
      },
      "source": [
        "**Stop words:**\n",
        "\n",
        "Stop words are those words in the text which does not add any meaning to the sentence and their removal will not affect the processing of text for the defined purpose. They are removed from the vocabulary to reduce noise and to reduce the dimension of the feature set\n",
        "\n",
        "    Different libraries to perform word tokenization:\n",
        "          NLTK, Spacy, Genism, Keras \n",
        "\n",
        "**Various Tokenization Techniques:**\n",
        "\n",
        "    1.Whitespace Tokenization\n",
        "This is the simplest tokenization technique. Given a sentence or paragraph it tokenizes into words by splitting the input wherever a white space in encountered. This is the fastest tokenization technique but will work for languages in which the white space breaks apart the sentence into meaningful words.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUBrrAkD96c3"
      },
      "source": [
        "# You can also import WhitespaceTokenizer() method from nltk\n",
        "from nltk.tokenize import WhitespaceTokenizer"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mP5Z2mvLAZsH",
        "outputId": "10198e7e-7d64-4135-bae7-e7e29a31c24e"
      },
      "source": [
        "wtk = WhitespaceTokenizer()\n",
        "\n",
        "#Give string input\n",
        "text3 = 'Natural Language Processing is a subset of Deep Learning'\n",
        "\n",
        "#use tokenize method\n",
        "tokens = wtk.tokenize(text3)\n",
        "print(tokens)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural', 'Language', 'Processing', 'is', 'a', 'subset', 'of', 'Deep', 'Learning']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naZxUmDRBKq7"
      },
      "source": [
        "    2.Dictionary Based Tokenization\n",
        "In this method the tokens are found based on the tokens already existing in the dictionary. If the token is not found, then special rules are used to tokenize it. It is an advanced technique compared to whitespace tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yaOH-SxArFO",
        "outputId": "c2cc17a2-e7c1-40f9-c3ec-688c2e300f4a"
      },
      "source": [
        "# create a list and dictionary variable\n",
        "lst = ['Spinach','Mango','Cashewnut']\n",
        "dct = {'vegetable':'Spinach','fruit':'Mango','dryfruit':'Cashewnut'}\n",
        "\n",
        "\n",
        "#extract words from the dictionary items\n",
        "word2index = {key:val for val, key in dct.items()}\n",
        "\n",
        "#print tokenized words\n",
        "tokenized_words = [[word2index[word] for word in text.split()] for text in lst] \n",
        "tokenized_words"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['vegetable'], ['fruit'], ['dryfruit']]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYlcw8gvDhx_"
      },
      "source": [
        "    3.Rule Based Tokenization\n",
        "In this technique a set of rules are created for the specific problem. The tokenization is done based on the rules. For example creating rules bases on grammar for particular language.\n",
        "\n",
        "    Regular Expression Tokenizer\n",
        "This technique uses regular expression to control the tokenization of text into tokens. Regular expression can be simple to complex and sometimes difficult to comprehend. This technique should be preferred when the above methods does not serve the required purpose. It is a rule based tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSvYNF76DHPJ",
        "outputId": "207899d0-e6dd-41a9-f171-5f24a1134c3b"
      },
      "source": [
        "# import RegexpTokenizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "tk = RegexpTokenizer(\"[\\w']+\")   #[\\w']+ is one type of regular expression which extracts whole words from text.\n",
        "\n",
        "#give an input string\n",
        "text4 = \"Let's see if we can work together.\"\n",
        "tokens = tk.tokenize(text4)\n",
        "tokens"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Let's\", 'see', 'if', 'we', 'can', 'work', 'together']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skcl2Ri9Fiwt"
      },
      "source": [
        "    Punctuation-based tokenizer\n",
        "Punctuation-based tokenization splits on whitespace and punctuations and also retains the punctuations.Punctuation-based tokenization overcomes the issue above and provides a meaningful token"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQVRYxxTEE_a",
        "outputId": "153956a3-6369-4fd0-9421-c942d4f4a102"
      },
      "source": [
        "#import wordpunct_tokenize form nltk library\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "\n",
        "text5 = 'Mr.Manoj buys property in: Hyderabad & Mumbai'\n",
        "tokens = wordpunct_tokenize(text5)\n",
        "tokens"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Mr', '.', 'Manoj', 'buys', 'property', 'in', ':', 'Hyderabad', '&', 'Mumbai']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DBV5fkeGoUJ"
      },
      "source": [
        "    Tweet Tokenizer\n",
        "Special texts, like Twitter tweets, have a characteristic structure and the generic tokenizers mentioned above fail to produce viable tokens when applied to these datasets. NLTK offers a special tokenizer for tweets to help in this case. This is a rule-based tokenizer that can remove HTML code, remove problematic characters, remove Twitter handles, and normalize text length by reducing the occurrence of repeated letters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xOCWhqtGZ-X",
        "outputId": "f5db8fe4-bcab-4781-f8b6-e66f1b04a1b0"
      },
      "source": [
        "#import TweetTokenizer from nltk\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "#create object of tokenizer\n",
        "tknzr = TweetTokenizer(strip_handles=True)\n",
        "tweet= \" @NLP_learner: NLP is way tooo coool:-) :-P <3\"\n",
        "\n",
        "x= tknzr.tokenize(tweet)\n",
        "print(x)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[':', 'NLP', 'is', 'way', 'tooo', 'coool', ':-)', ':-P', '<3']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVi5xFiyHxdv"
      },
      "source": [
        "    MWE(Multi-Word Expression) Tokenizer\n",
        "The multi-word expression tokenizer is a rule-based, “add-on” tokenizer offered by NLTK. Once the text has been tokenized by a tokenizer of choice, some tokens can be re-grouped into multi-word expressions.\n",
        "\n",
        "MWE Tokenizer takes a string and merges multi-word expressions into single tokens, using a lexicon of MWEs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuopltgPHZT5",
        "outputId": "18c27073-c24e-4765-bdf2-75d9b3bccda9"
      },
      "source": [
        "from nltk.tokenize import MWETokenizer\n",
        "   \n",
        "# Create a reference variable for Class MWETokenizer\n",
        "tk = MWETokenizer([('M', 'W', 'E'), ('Multi', 'Word', 'Tokenier')])\n",
        "tk.add_mwe(('Natural', 'Language', 'Processing'))\n",
        "   \n",
        "# Create a string input\n",
        "text = \"What is M W E in Natural Language Processing\"\n",
        "   \n",
        "# Use tokenize method\n",
        "tokenized = tk.tokenize(text.split())\n",
        "   \n",
        "print(tokenized)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['What', 'is', 'M_W_E', 'in', 'Natural_Language_Processing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5al8unG4IVsG"
      },
      "source": [
        "    4.Penn TreeBank/Default Tokenization\n",
        "Tree bank is a corpus created which gives the semantic and syntactical annotation of language. Penn Treebank is one of the largest treebanks which was published. This technique of tokenization separates the punctuation, clitics (words that occur along with other words like I’m, don’t) and hyphenated words together."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOevjb1PH98z",
        "outputId": "ab35184f-f65b-4a19-b64f-f697a8c62201"
      },
      "source": [
        "#import tokenizer from nltk\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "#create object of TreebankWordTokenizer\n",
        "tk = TreebankWordTokenizer()\n",
        "text = \"That's True, Mr. Manoj Singh.\"\n",
        "tokens = tk.tokenize(text)\n",
        "tokens"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['That', \"'s\", 'True', ',', 'Mr.', 'Manoj', 'Singh', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-YE56oDI3_v"
      },
      "source": [
        "    5.Spacy Tokenizer\n",
        "This is a modern technique of tokenization which is faster and easily customizable. It provides the flexibility to specify special tokens that need not be segmented or need to be segmented using special rules. Suppose you want to keep $ as a separate token, it takes precedence over other tokenization operations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uFS_o40IlNy"
      },
      "source": [
        "import spacy\n",
        "from spacy.lang.en import English\n",
        "\n",
        "# Load English tokenizer, tagger, parser, NER and word vectors by creating object\n",
        "nlp = English()\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "025mRHMvJBco"
      },
      "source": [
        "# give an input string with multiple sentences\n",
        "string = \"\"\"Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet \n",
        "species by building a self-sustaining city on Mars. In 2008, SpaceX’s Falcon 1 became the first privately developed \n",
        "liquid-fuel launch vehicle to orbit the Earth.\"\"\""
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_yzRJWaJSkZ",
        "outputId": "56976888-ad51-41a5-9d11-f69c2227d234"
      },
      "source": [
        "text = nlp(string)\n",
        "\n",
        "# Create list of word tokens\n",
        "token_list = []\n",
        "\n",
        "for token in text:\n",
        "  token_list.append(token.text)\n",
        "token_list "
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Founded',\n",
              " 'in',\n",
              " '2002',\n",
              " ',',\n",
              " 'SpaceX',\n",
              " '’s',\n",
              " 'mission',\n",
              " 'is',\n",
              " 'to',\n",
              " 'enable',\n",
              " 'humans',\n",
              " 'to',\n",
              " 'become',\n",
              " 'a',\n",
              " 'spacefaring',\n",
              " 'civilization',\n",
              " 'and',\n",
              " 'a',\n",
              " 'multi',\n",
              " '-',\n",
              " 'planet',\n",
              " '\\n',\n",
              " 'species',\n",
              " 'by',\n",
              " 'building',\n",
              " 'a',\n",
              " 'self',\n",
              " '-',\n",
              " 'sustaining',\n",
              " 'city',\n",
              " 'on',\n",
              " 'Mars',\n",
              " '.',\n",
              " 'In',\n",
              " '2008',\n",
              " ',',\n",
              " 'SpaceX',\n",
              " '’s',\n",
              " 'Falcon',\n",
              " '1',\n",
              " 'became',\n",
              " 'the',\n",
              " 'first',\n",
              " 'privately',\n",
              " 'developed',\n",
              " '\\n',\n",
              " 'liquid',\n",
              " '-',\n",
              " 'fuel',\n",
              " 'launch',\n",
              " 'vehicle',\n",
              " 'to',\n",
              " 'orbit',\n",
              " 'the',\n",
              " 'Earth',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yaA_HTAKALt"
      },
      "source": [
        "    6.Moses Tokenizer\n",
        "This is a tokenizer which is advanced and is available before Spacy was introduced. It is basically a collection of complex normalization and segmentation logic which works very well for structured language like English."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5inUyydJuX2",
        "outputId": "563a9e14-cf76-4fbd-e60d-8b23ae59ae9a"
      },
      "source": [
        "!pip install sacremoses\n",
        "from sacremoses import MosesTokenizer"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[?25l\r\u001b[K     |▍                               | 10 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |▊                               | 20 kB 31.7 MB/s eta 0:00:01\r\u001b[K     |█                               | 30 kB 21.7 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 40 kB 16.9 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 51 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 61 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 71 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███                             | 81 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 92 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 102 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████                            | 112 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 122 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 133 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 143 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 153 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 163 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 174 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 184 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████                         | 194 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 204 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 215 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████                        | 225 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 235 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 245 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 256 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 266 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 276 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 286 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 296 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 307 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 317 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 327 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 337 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 348 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 358 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 368 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 378 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 389 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 399 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 409 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 419 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 430 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 440 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 450 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 460 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 471 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 481 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 491 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 501 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 512 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 522 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 532 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 542 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 552 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 563 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 573 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 583 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 593 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 604 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 614 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 624 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 634 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 645 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 655 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 665 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 675 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 686 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 696 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 706 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 716 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 727 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 737 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 747 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 757 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 768 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 778 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 788 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 798 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 808 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 819 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 829 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 839 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 849 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 860 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 870 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 880 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 890 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 895 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sacremoses) (4.62.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.0.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacremoses) (2019.12.20)\n",
            "Installing collected packages: sacremoses\n",
            "Successfully installed sacremoses-0.0.46\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2C8aojtKGBk",
        "outputId": "c1bf672f-372b-4472-dca9-70fb076c45a7"
      },
      "source": [
        "#create tokenizer object\n",
        "mt = MosesTokenizer()\n",
        "text = \"okay,Let me check Moses Tokenizer, Mr.Vishal\"\n",
        "tokens = mt.tokenize(text)\n",
        "tokens"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['okay', ',', 'Let', 'me', 'check', 'Moses', 'Tokenizer', ',', 'Mr.Vishal']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYUQSPo_K14B"
      },
      "source": [
        "---------------\n",
        "---------------"
      ]
    }
  ]
}