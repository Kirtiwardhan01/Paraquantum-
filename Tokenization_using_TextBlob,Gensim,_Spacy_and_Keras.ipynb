{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tokenization using TextBlob,Gensim, Spacy. and Keras.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM3MuZCI2+6Z6bzleZSuho/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kirtiwardhan01/Paraquantum-/blob/main/Tokenization_using_TextBlob%2CGensim%2C_Spacy_and_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jugy5Bk1OJlR"
      },
      "source": [
        "TextBlob module is a Python library and offers a simple API to access its methods and perform basic NLP tasks. It is built on the top of NLTK module.\n",
        "\n",
        "    Install TextBlob using the following commands in terminal:\n",
        "\n",
        "    pip install -U textblob\n",
        "    python -m textblob.download_corpora\n",
        "\n",
        "Some terms that will be frequently used are :\n",
        "\n",
        "    Corpus – Body of text, singular. Corpora is the plural of this.\n",
        "    Lexicon – Words and their meanings.\n",
        "    Token – Each “entity” that is a part of whatever was split up based on rules. \n",
        "For examples, each word is a token when a sentence is “tokenized” into words. Each sentence can also be a token, if you tokenized the sentences out of a paragraph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMVQQEfgLdwH",
        "outputId": "5815e3fa-6cf3-4e2f-de72-cb6004fdaefa"
      },
      "source": [
        "pip install -U textblob"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textblob) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyjFsP9ZOjKY",
        "outputId": "c3186923-78c7-46b2-ce69-083d759cefa9"
      },
      "source": [
        "# from textblob library import TextBlob method\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import textblob\n",
        "from textblob import TextBlob"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDikY3S1Ox_E"
      },
      "source": [
        "text = ('We will discuss several common ways to deal with text data. We will discuss different feature extraction methods. We will start with some basic techniques which will lead into advanced Natural Language Processing techniques. We will also learn about pre-processing of the text data in order to extract better features from clean data')\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZTdlmr3PV9J",
        "outputId": "25d326db-eab8-4094-9867-e6fbe43aa775"
      },
      "source": [
        "# create a TextBlob object\n",
        "blob_object = TextBlob(text)\n",
        "\n",
        "#tokenize paragraph into words\n",
        "print('Word Tokenize:\\n',blob_object.words)\n",
        "print('\\n')\n",
        "#tokenize paragraph into sentences\n",
        "print('Sentence Tokenize:\\n',blob_object.sentences)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Tokenize:\n",
            " ['We', 'will', 'discuss', 'several', 'common', 'ways', 'to', 'deal', 'with', 'text', 'data', 'We', 'will', 'discuss', 'different', 'feature', 'extraction', 'methods', 'We', 'will', 'start', 'with', 'some', 'basic', 'techniques', 'which', 'will', 'lead', 'into', 'advanced', 'Natural', 'Language', 'Processing', 'techniques', 'We', 'will', 'also', 'learn', 'about', 'pre-processing', 'of', 'the', 'text', 'data', 'in', 'order', 'to', 'extract', 'better', 'features', 'from', 'clean', 'data']\n",
            "\n",
            "\n",
            "Sentence Tokenize:\n",
            " [Sentence(\"We will discuss several common ways to deal with text data.\"), Sentence(\"We will discuss different feature extraction methods.\"), Sentence(\"We will start with some basic techniques which will lead into advanced Natural Language Processing techniques.\"), Sentence(\"We will also learn about pre-processing of the text data in order to extract better features from clean data\")]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EY0wwC8nQ8RO"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67WyaL9BQ9bz"
      },
      "source": [
        "    Gensim word tokenizer\n",
        "Gensim is a Python library for topic modeling, document indexing, and similarity retrieval with large corpora. The target audience is the natural language processing (NLP) and information retrieval (IR) community. It offers utility functions for tokenization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiIB35ITP0K6",
        "outputId": "384820a9-4a9f-4010-e514-fd579e05adee"
      },
      "source": [
        "#importing relevant genism library  \n",
        "import gensim\n",
        "from gensim.utils import tokenize\n",
        "\n",
        "list(tokenize(text))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['We',\n",
              " 'will',\n",
              " 'discuss',\n",
              " 'several',\n",
              " 'common',\n",
              " 'ways',\n",
              " 'to',\n",
              " 'deal',\n",
              " 'with',\n",
              " 'text',\n",
              " 'data',\n",
              " 'We',\n",
              " 'will',\n",
              " 'discuss',\n",
              " 'different',\n",
              " 'feature',\n",
              " 'extraction',\n",
              " 'methods',\n",
              " 'We',\n",
              " 'will',\n",
              " 'start',\n",
              " 'with',\n",
              " 'some',\n",
              " 'basic',\n",
              " 'techniques',\n",
              " 'which',\n",
              " 'will',\n",
              " 'lead',\n",
              " 'into',\n",
              " 'advanced',\n",
              " 'Natural',\n",
              " 'Language',\n",
              " 'Processing',\n",
              " 'techniques',\n",
              " 'We',\n",
              " 'will',\n",
              " 'also',\n",
              " 'learn',\n",
              " 'about',\n",
              " 'pre',\n",
              " 'processing',\n",
              " 'of',\n",
              " 'the',\n",
              " 'text',\n",
              " 'data',\n",
              " 'in',\n",
              " 'order',\n",
              " 'to',\n",
              " 'extract',\n",
              " 'better',\n",
              " 'features',\n",
              " 'from',\n",
              " 'clean',\n",
              " 'data']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSv_s3FuRuWH"
      },
      "source": [
        "    spaCy Tokenizer\n",
        "SpaCy is an open-source Python library that parses and understands large volumes of text. With available models catering to specific languages (English, French, German, etc.), it handles NLP tasks with the most efficient implementation of common algorithms.\n",
        "\n",
        "spaCy tokenizer provides the flexibility to specify special tokens that don’t need to be segmented, or need to be segmented using special rules for each language, for example punctuation at the end of a sentence should be split off – whereas “U.K.” should remain one token.\n",
        "\n",
        "Before you can use spaCy you need to install it, download data and models for the English language"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmyNPzXSRApe"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jYiHtt1R0pw",
        "outputId": "89659811-290c-4795-f509-a69cec992c41"
      },
      "source": [
        "doc = nlp(text)\n",
        "\n",
        "for token in doc:\n",
        "  print(token,token.idx)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We 0\n",
            "will 3\n",
            "discuss 8\n",
            "several 16\n",
            "common 24\n",
            "ways 31\n",
            "to 36\n",
            "deal 39\n",
            "with 44\n",
            "text 49\n",
            "data 54\n",
            ". 58\n",
            "We 60\n",
            "will 63\n",
            "discuss 68\n",
            "different 76\n",
            "feature 86\n",
            "extraction 94\n",
            "methods 105\n",
            ". 112\n",
            "We 114\n",
            "will 117\n",
            "start 122\n",
            "with 128\n",
            "some 133\n",
            "basic 138\n",
            "techniques 144\n",
            "which 155\n",
            "will 161\n",
            "lead 166\n",
            "into 171\n",
            "advanced 176\n",
            "Natural 185\n",
            "Language 193\n",
            "Processing 202\n",
            "techniques 213\n",
            ". 223\n",
            "We 225\n",
            "will 228\n",
            "also 233\n",
            "learn 238\n",
            "about 244\n",
            "pre 250\n",
            "- 253\n",
            "processing 254\n",
            "of 265\n",
            "the 268\n",
            "text 272\n",
            "data 277\n",
            "in 282\n",
            "order 285\n",
            "to 291\n",
            "extract 294\n",
            "better 302\n",
            "features 309\n",
            "from 318\n",
            "clean 323\n",
            "data 329\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sJmQJR9UiAY",
        "outputId": "c21c8a56-7fe0-45e1-f3f9-d5d7528b2c6f"
      },
      "source": [
        "for token in doc:\n",
        "  print(token,token.pos,token.pos_)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We 95 PRON\n",
            "will 100 VERB\n",
            "discuss 100 VERB\n",
            "several 84 ADJ\n",
            "common 84 ADJ\n",
            "ways 92 NOUN\n",
            "to 94 PART\n",
            "deal 100 VERB\n",
            "with 85 ADP\n",
            "text 92 NOUN\n",
            "data 92 NOUN\n",
            ". 97 PUNCT\n",
            "We 95 PRON\n",
            "will 100 VERB\n",
            "discuss 100 VERB\n",
            "different 84 ADJ\n",
            "feature 92 NOUN\n",
            "extraction 92 NOUN\n",
            "methods 92 NOUN\n",
            ". 97 PUNCT\n",
            "We 95 PRON\n",
            "will 100 VERB\n",
            "start 100 VERB\n",
            "with 85 ADP\n",
            "some 90 DET\n",
            "basic 84 ADJ\n",
            "techniques 92 NOUN\n",
            "which 90 DET\n",
            "will 100 VERB\n",
            "lead 100 VERB\n",
            "into 85 ADP\n",
            "advanced 84 ADJ\n",
            "Natural 96 PROPN\n",
            "Language 96 PROPN\n",
            "Processing 92 NOUN\n",
            "techniques 92 NOUN\n",
            ". 97 PUNCT\n",
            "We 95 PRON\n",
            "will 100 VERB\n",
            "also 86 ADV\n",
            "learn 100 VERB\n",
            "about 85 ADP\n",
            "pre 84 ADJ\n",
            "- 92 NOUN\n",
            "processing 92 NOUN\n",
            "of 85 ADP\n",
            "the 90 DET\n",
            "text 92 NOUN\n",
            "data 92 NOUN\n",
            "in 85 ADP\n",
            "order 92 NOUN\n",
            "to 94 PART\n",
            "extract 100 VERB\n",
            "better 84 ADJ\n",
            "features 92 NOUN\n",
            "from 85 ADP\n",
            "clean 84 ADJ\n",
            "data 92 NOUN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJWVPXIUVXdQ"
      },
      "source": [
        "    Tokenization with Keras\n",
        "Keras open-source library is one of the most reliable deep learning frameworks. To perform tokenization we use: **text_to_word_sequence** method from the Class **Keras.preprocessing.text class**. The great thing about Keras is converting the alphabet in a lower case before tokenizing it, which can be quite a time-saver."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-AJJ_zLVDKc"
      },
      "source": [
        "#Importing library\n",
        "import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.text import text_to_word_sequence"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dj4MmJN3Vlb9",
        "outputId": "b2a6606e-eeef-43b1-b85a-63abe836155d"
      },
      "source": [
        "#Creating the object\n",
        "ntoken = Tokenizer()\n",
        "ntoken.fit_on_texts(text)\n",
        "\n",
        "list_of_words = text_to_word_sequence(text)\n",
        "print(list_of_words)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['we', 'will', 'discuss', 'several', 'common', 'ways', 'to', 'deal', 'with', 'text', 'data', 'we', 'will', 'discuss', 'different', 'feature', 'extraction', 'methods', 'we', 'will', 'start', 'with', 'some', 'basic', 'techniques', 'which', 'will', 'lead', 'into', 'advanced', 'natural', 'language', 'processing', 'techniques', 'we', 'will', 'also', 'learn', 'about', 'pre', 'processing', 'of', 'the', 'text', 'data', 'in', 'order', 'to', 'extract', 'better', 'features', 'from', 'clean', 'data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiJnwp92WhEu"
      },
      "source": [
        "----------\n",
        "--------\n",
        "----------"
      ]
    }
  ]
}