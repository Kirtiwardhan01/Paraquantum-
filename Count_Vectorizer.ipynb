{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Count Vectorizer.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOAJqZYcjCAslld3oEibV9J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kirtiwardhan01/Paraquantum-/blob/main/Count_Vectorizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vzuffnw8i0km"
      },
      "source": [
        "In this notebook we'll see how sklearn library CountVectorizer can be used in converting human readable English text into a language machine can understand\n",
        "\n",
        "Let’s begin with a very simple text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1qcEjYkQ_h9"
      },
      "source": [
        "text=[\"Vishal's family includes 3 kids a dog and 2 cats\", \n",
        "      \"The cats are friendly.The dog is beautiful\",\n",
        "      \"The cat is 11 years old\",\n",
        "      \"Vishal lives in the United States of America\"]\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0JuZaGcjYJb"
      },
      "source": [
        "How can we make machine understand this?\n",
        "\n",
        "####**Token**\n",
        "\n",
        "First step is to take the text and break it into individual words (tokens). We are going to use sklearn library for this.\n",
        "\n",
        "Import CountVectorizer class from feature_extraction.text library of sklearn. Create an instance of CountVectorizer and fit the instance with the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hmqm72xWjU8O",
        "outputId": "65810c06-3a7f-41c8-f7c4-388d444c4ce6"
      },
      "source": [
        "import sklearn\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(text)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
              "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
              "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
              "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                tokenizer=None, vocabulary=None)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTAWqxjIkT74"
      },
      "source": [
        "Notice that by default lowercase is True\n",
        "\n",
        "That means, while creating tokens, CountVectorizer will change all the words in lowercase.\n",
        "\n",
        "Call **get_feature_names function** to get the list of tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZbFOH20jmFV",
        "outputId": "e5ca3954-052c-4925-f5aa-662b3bb5c12c"
      },
      "source": [
        "print(vectorizer.get_feature_names())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['11', 'america', 'and', 'are', 'beautiful', 'cat', 'cats', 'dog', 'family', 'friendly', 'in', 'includes', 'is', 'kids', 'lives', 'of', 'old', 'states', 'the', 'united', 'vishal', 'years']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjgpwApmkubg"
      },
      "source": [
        "Make note of few things here\n",
        "\n",
        "a. As mentioned all the tokens are in small case\n",
        "\n",
        "b. Single character words like numbers 3, 1, 2 are not there in the list of tokens/features. This is happening because, by default CountVectorizer considers only words with at least two characters. If you want to include single character words, you need to change the token_pattern option.\n",
        "\n",
        "c. Instead of Vishal’s the token has vishal. That means the punctuation is not considered.\n",
        "\n",
        "d. United States of America is tokenized as four different words. What if you want to tell Machine to check if a document contains United States of America as a country? We will handle this scenario later in this notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6ZPjYEukcBb",
        "outputId": "43c12106-b6c8-4cfd-f85c-4bf7fa3e8257"
      },
      "source": [
        "vectorizer=CountVectorizer(token_pattern=u'(?u)\\\\b\\\\w+\\\\b')\n",
        "vectorizer.fit(text)\n",
        "print(vectorizer.get_feature_names())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['11', '2', '3', 'a', 'america', 'and', 'are', 'beautiful', 'cat', 'cats', 'dog', 'family', 'friendly', 'in', 'includes', 'is', 'kids', 'lives', 'of', 'old', 's', 'states', 'the', 'united', 'vishal', 'years']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfnNtp-9lVLT"
      },
      "source": [
        "In real world project, when your model has to read several thousand documents to categorize or humongous number of emails to filter spam emails regular words like ‘is’, ‘the’, ‘a’, ‘an’, ‘and’, ‘are’ don’t add any value. It is better to not tokenize them. \n",
        "\n",
        "In order to not consider them use the option stop_words as shown below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gka7COEvlB5B",
        "outputId": "fce82305-8268-46fd-9b32-8145c8207812"
      },
      "source": [
        "vectorizer=CountVectorizer(token_pattern=u'(?u)\\\\b\\\\w+\\\\b',stop_words=['is','are','and','in','the'])\n",
        "vectorizer.fit(text)\n",
        "print(vectorizer.get_feature_names())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['11', '2', '3', 'a', 'america', 'beautiful', 'cat', 'cats', 'dog', 'family', 'friendly', 'includes', 'kids', 'lives', 'of', 'old', 's', 'states', 'united', 'vishal', 'years']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osy9RcqpllZ4"
      },
      "source": [
        "So far the tokens are still human readable text. Machine cannot understand them. These tokens have to be encoded. This encoding is done by CountVectorizer while fitting the text. You can see the encoded values by calling vocabulary_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62CEeB7zlbfr",
        "outputId": "56c03785-de94-46ee-b522-db02aaf60d6e"
      },
      "source": [
        "print(vectorizer.vocabulary_)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'vishal': 19, 's': 16, 'family': 9, 'includes': 11, '3': 2, 'kids': 12, 'a': 3, 'dog': 8, '2': 1, 'cat': 6, 'cats': 7, 'friendly': 10, 'beautiful': 5, '11': 0, 'years': 20, 'old': 15, 'lives': 13, 'united': 18, 'states': 17, 'of': 14, 'america': 4}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6njf9DBl7_L"
      },
      "source": [
        "There are twenty one tokens encoded with numbers ranging from 0 to 20. The sequence of words in text is not considered while encoding the tokens.\n",
        "\n",
        "**Frequency**\n",
        "\n",
        "After tokens and encoding comes counting how many times each word appeared in the text.\n",
        "\n",
        "Call transform method of CountVectorizer instance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvTjHQuQlpiV",
        "outputId": "22b5009d-4675-4480-d8d9-77bd749a46d3"
      },
      "source": [
        "vector = vectorizer.transform(text)\n",
        "vector"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<4x21 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 24 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzjgD_2QmES0",
        "outputId": "0068e80d-ce2e-4d35-997d-b1ca8ebf7c26"
      },
      "source": [
        "#Check the shape of vector\n",
        "print(vector.shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4, 21)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8gFWXVhqfpJ"
      },
      "source": [
        "The text has been encoded into sparse matrix. Convert this sparse matrix into numpy array by calling toarray method.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDrP_5NGqYWr",
        "outputId": "c39dc218-a705-4d9f-96a0-e4fcac50cdbf"
      },
      "source": [
        "vector.toarray()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0],\n",
              "       [0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1],\n",
              "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0a9E_tnqvwd"
      },
      "source": [
        "There are twenty four (representing twenty four words considered) ‘1’ spread across four rows. Each one represent a word in the text as per encoded value. \n",
        "\n",
        "To understand this array better, convert it into pandas dataframe\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "nRvrLih2qhgx",
        "outputId": "0b6d9d8b-23c0-4c73-db09-374975f2d76c"
      },
      "source": [
        "import pandas as pd\n",
        "pd.DataFrame(vector.toarray(),columns=vectorizer.get_feature_names())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>11</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>a</th>\n",
              "      <th>america</th>\n",
              "      <th>beautiful</th>\n",
              "      <th>cat</th>\n",
              "      <th>cats</th>\n",
              "      <th>dog</th>\n",
              "      <th>family</th>\n",
              "      <th>friendly</th>\n",
              "      <th>includes</th>\n",
              "      <th>kids</th>\n",
              "      <th>lives</th>\n",
              "      <th>of</th>\n",
              "      <th>old</th>\n",
              "      <th>s</th>\n",
              "      <th>states</th>\n",
              "      <th>united</th>\n",
              "      <th>vishal</th>\n",
              "      <th>years</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   11  2  3  a  america  beautiful  ...  old  s  states  united  vishal  years\n",
              "0   0  1  1  1        0          0  ...    0  1       0       0       1      0\n",
              "1   0  0  0  0        0          1  ...    0  0       0       0       0      0\n",
              "2   1  0  0  0        0          0  ...    1  0       0       0       0      1\n",
              "3   0  0  0  0        1          0  ...    0  0       1       1       1      0\n",
              "\n",
              "[4 rows x 21 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIyPdapqrP4s"
      },
      "source": [
        "Now compare the token numbers with the values to understand which word appeared how many times.\n",
        "\n",
        "For example token 0 is for number 1. Number 1 appeared once in first row, hence it shows 1 for column 0 and row 0 in above data frame. For all other rows column 0 contains 0.\n",
        "\n",
        "Vocabulary\n",
        "\n",
        "Now coming back to the scenario we discussed in point d of Tokenization section. Suppose you are only interested in knowing whether or not the text contain United States of America.\n",
        "\n",
        "You can achieve this by using ngram_range and vocabulary option.\n",
        "\n",
        "ngram_range indicates how many words will be considered together for tokenization. Default value is [1,1], that’s why it considered every word as new token.\n",
        "\n",
        "Run below line of code to understand how ngram_range works.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJlxuSTgqytc",
        "outputId": "1d0a2572-441b-49ef-946f-f7bc6cf9a8c9"
      },
      "source": [
        "vectorizer=CountVectorizer(token_pattern=u'(?u)\\\\b\\\\w+\\\\b',stop_words=['is','are','and','in','the'],ngram_range=[1,4])\n",
        "vectorizer.fit(text)\n",
        "print(vectorizer.get_feature_names())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['11', '11 years', '11 years old', '2', '2 cat', '3', '3 kids', '3 kids a', '3 kids a dog', 'a', 'a dog', 'a dog 2', 'a dog 2 cat', 'america', 'beautiful', 'cat', 'cat 11', 'cat 11 years', 'cat 11 years old', 'cats', 'cats friendly', 'cats friendly dog', 'cats friendly dog beautiful', 'dog', 'dog 2', 'dog 2 cat', 'dog beautiful', 'family', 'family includes', 'family includes 3', 'family includes 3 kids', 'friendly', 'friendly dog', 'friendly dog beautiful', 'includes', 'includes 3', 'includes 3 kids', 'includes 3 kids a', 'kids', 'kids a', 'kids a dog', 'kids a dog 2', 'lives', 'lives united', 'lives united states', 'lives united states of', 'of', 'of america', 'old', 's', 's family', 's family includes', 's family includes 3', 'states', 'states of', 'states of america', 'united', 'united states', 'united states of', 'united states of america', 'vishal', 'vishal lives', 'vishal lives united', 'vishal lives united states', 'vishal s', 'vishal s family', 'vishal s family includes', 'years', 'years old']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPFDHzKQrkR8"
      },
      "source": [
        "ngram_range is mentioned as 1 to 4, hence CountVectorizer considers single word to four word combination as separate token. \n",
        "\n",
        "Now if you add vocabulary option to this, it will meet the requirement.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUn3m6h9rWdr",
        "outputId": "e393c0bd-8d21-450d-e919-2325da906a20"
      },
      "source": [
        "vectorizer=CountVectorizer(token_pattern=u'(?u)\\\\b\\\\w+\\\\b',stop_words=['is','are','and','in','the'],ngram_range=[1,4],vocabulary=['united states of america'])\n",
        "vectorizer.fit(text)\n",
        "print(vectorizer.get_feature_names())"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['united states of america']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmRPQtHaruLK"
      },
      "source": [
        "It returns only one token as it found the word mentioned in vocabulary.\n",
        "If the text doesn’t contain united states of america, it will not return any thing. This way you can filter a document based on content you are looking for."
      ]
    }
  ]
}