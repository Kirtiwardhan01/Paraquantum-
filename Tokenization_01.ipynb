{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tokenization 01.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8IRs8fxgc__"
      },
      "source": [
        "###Tokenization\n",
        "\n",
        "Well, we'll see how to represent the words in a way that the computers can process them with a view to later training the machine that can understand their meaning. Tokenization is the process of tokenizing or splitting a string, text into a list of tokens\n",
        "\n",
        "So let's consider a word 'SILENT'. \n",
        "\n",
        "It's made of sequence of letters. These letters are can be represented by numbers using an encoding scheme. A popular one called ASCII has these letters represented by these numbers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "OmoXXpiDhZVr",
        "outputId": "4a64705b-ae00-4761-f4a7-ef357da954ed"
      },
      "source": [
        "#So each letter from the word SILENT can be represented in numbers as follows:\n",
        "\n",
        "print(ord('S'))     \n",
        "print(ord('I'))\n",
        "print(ord('L'))\n",
        "print(ord('E'))\n",
        "print(ord('N'))\n",
        "print(ord('T'))\n",
        "\n",
        "\"\"\"So the above bunch of numbers collectively represent a word SILENT\"\"\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "83\n",
            "73\n",
            "76\n",
            "69\n",
            "78\n",
            "84\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'So the above bunch of numbers collectively represent a word SILENT'"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KY68GUCYkY0G"
      },
      "source": [
        "**But the word \"LISTEN\" has the same letters and thus the numbers but in different order. So it makes hard for us to understand the sentiment of the word just by the letters in it**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zM0pFk-Ni4Ha",
        "outputId": "5b4bb115-cb48-45fe-c837-e42862e58a2f"
      },
      "source": [
        "#So we can see how both the words with same letters and numbers are represented in different orders:\n",
        "print('SILENT:', \"S:\",ord('S'),\"I:\",ord('I'),\"L:\",ord('L'),\"E:\",ord('E'),\"N:\",ord('N'),\"T:\",ord('T')) \n",
        "print('LISTEN:', \"L:\",ord('L'),\"I:\",ord('I'),\"S:\",ord('S'),\"T:\",ord('T'),\"E:\",ord('E'),\"N:\",ord('N')) "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SILENT: S: 83 I: 73 L: 76 E: 69 N: 78 T: 84\n",
            "LISTEN: L: 76 I: 73 S: 83 T: 84 E: 69 N: 78\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7ffJtyhoIIf"
      },
      "source": [
        "**So it might be easier to encode words than encoding letters.** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "w-gMW0SonV8j",
        "outputId": "7615d4b7-d59d-463d-8179-650ca974eb70"
      },
      "source": [
        "#Let's consider the sentence \n",
        "\"I love playing cricket.\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'I love playing cricket.'"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjEwEKTfo4F6"
      },
      "source": [
        "**So what will happen if we start encoding the words in the sentence instead of encoding the letters in each words?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oo_hAXryvAJ0"
      },
      "source": [
        "#Let's import relevant libraries and packages needed \n",
        "import spacy"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HD_xFuYBv0Y7"
      },
      "source": [
        "from spacy.tokenizer import Tokenizer\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pirmQnNZwgRc",
        "outputId": "8ec5db95-532b-4861-dfc8-940fbee01550"
      },
      "source": [
        "#Let's create a text\n",
        "\n",
        "txt = 'I love playing cricket'\n",
        "print(txt)\n",
        "\n",
        "text2 = nlp(\"I love playing football\")\n",
        "print(text2)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I love playing cricket\n",
            "I love playing football\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaTPaGycyWVd"
      },
      "source": [
        "#Let's create a document of the text and convert the sentence into tokens\n",
        "doc= nlp(txt)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eqz0w2rgydeH",
        "outputId": "f86df609-687c-4274-cd87-98cbdf43b5e0"
      },
      "source": [
        "for token in doc:\n",
        "  print(token.text)\n",
        "  #check the no. of words in the sentences\n",
        "print(len(doc))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I\n",
            "love\n",
            "playing\n",
            "cricket\n",
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88D3wmVqytzH"
      },
      "source": [
        "##Let's convert the sentence into tokens\n",
        "doc2 = nlp(\"We're here to help you, Send us queries on john01@hotmail.com email id or visit us on https://www.w3schools.com!\" )"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTfbLpFrzqWi",
        "outputId": "aa5d00f0-303a-4bb0-cf41-27027c05fb72"
      },
      "source": [
        "for token in doc2:\n",
        "  print(token.text)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We\n",
            "'re\n",
            "here\n",
            "to\n",
            "help\n",
            "you\n",
            ",\n",
            "Send\n",
            "us\n",
            "queries\n",
            "on\n",
            "john01@hotmail.com\n",
            "email\n",
            "i\n",
            "d\n",
            "or\n",
            "visit\n",
            "us\n",
            "on\n",
            "https://www.w3schools.com\n",
            "!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRFmjtrdzvHJ"
      },
      "source": [
        "#Let's see below examples that has special characters and how we can tokenize them\n",
        "doc3 = nlp(\"A 10km ride in Hyderabad costs Rs.100\")\n",
        "doc4 = nlp(\"A 10km ride in Hyderabad costs $100\")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srpu-Qmo0LR8",
        "outputId": "57c7cad7-e185-464b-81cd-69fe798a4a02"
      },
      "source": [
        "for t in doc3:\n",
        "  print(t)\n",
        "print('\\n')\n",
        "\n",
        "for t in doc4:\n",
        "  print(t)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A\n",
            "10\n",
            "km\n",
            "ride\n",
            "in\n",
            "Hyderabad\n",
            "costs\n",
            "Rs.100\n",
            "\n",
            "\n",
            "A\n",
            "10\n",
            "km\n",
            "ride\n",
            "in\n",
            "Hyderabad\n",
            "costs\n",
            "$\n",
            "100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maVGfdnr0Tdi",
        "outputId": "b563c928-dc59-4ea6-ac35-10aa18c83f04"
      },
      "source": [
        "#Let's check the no. of words in the sentences\n",
        "print(len(doc))\n",
        "print(len(doc2))\n",
        "print(len(doc3))\n",
        "print(len(doc4))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n",
            "21\n",
            "8\n",
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuc_5iiq055y",
        "outputId": "ac5d2d4f-ac7d-4926-a4a3-a8c2fa4755e2"
      },
      "source": [
        "#Let's check the no. of vocab in english lang.\n",
        "print(len(doc.vocab))\n",
        "print(len(doc2.vocab))\n",
        "print(len(doc3.vocab))\n",
        "print(len(doc4.vocab))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "505\n",
            "505\n",
            "505\n",
            "505\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gc0HbZ4I1FBZ"
      },
      "source": [
        "###Tokenization using Indexing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0j9hhqEj0_BE"
      },
      "source": [
        "doc5 = nlp(u\"Let's import the packages\")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DSrYQey1RGy",
        "outputId": "13b7a448-e2a3-46f7-aca5-6646d575af21"
      },
      "source": [
        "#Using indexing \n",
        "print(doc5[0])\n",
        "\n",
        "#Using slice indexing\n",
        "print(doc5[0:4])\n",
        "print(doc5[1:4])\n",
        "print(doc5[2:5])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Let\n",
            "Let's import the\n",
            "'s import the\n",
            "import the packages\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "y2OPToip1VYb",
        "outputId": "b9352e8d-8447-4a11-c16b-7dfac4f013d3"
      },
      "source": [
        "doc5[0] = \"It's\" #spacy.tokens.doc.Doc' object does not support item assignment."
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-b3f363d0ddfe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdoc5\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"It's\"\u001b[0m \u001b[0;31m#spacy.tokens.doc.Doc' object does not support item assignment.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'spacy.tokens.doc.Doc' object does not support item assignment"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HSj08_r1hd0"
      },
      "source": [
        "doc6 = nlp(u\"TCS has commissioned its new campus in Pune at a cost of $10 million.\")"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDQVI19p1oa8",
        "outputId": "55267d15-4fe7-41f8-f007-5f1375a6d9d9"
      },
      "source": [
        "#Let's tokenize\n",
        "for token in doc6:\n",
        "  print(token.text, end=' | ')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TCS | has | commissioned | its | new | campus | in | Pune | at | a | cost | of | $ | 10 | million | . | "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tNq4Zyr1sHG",
        "outputId": "1515c549-de38-43c6-9adc-98ad61ea7184"
      },
      "source": [
        "#We can use NER as there are few entities\n",
        "for entity in doc6.ents:\n",
        "  print(entity)\n",
        "  print(entity.label_)\n",
        "  print(str(spacy.explain(entity.label_)))         #Explains where the entity belongs to  \n",
        "  print('\\n')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TCS\n",
            "ORG\n",
            "Companies, agencies, institutions, etc.\n",
            "\n",
            "\n",
            "Pune\n",
            "GPE\n",
            "Countries, cities, states\n",
            "\n",
            "\n",
            "$10 million\n",
            "MONEY\n",
            "Monetary values, including unit\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}